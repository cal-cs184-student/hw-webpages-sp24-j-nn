<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Project 3: Pathtracer</title>
<style>
  body {
    margin: 20px;
  }
  h1, h2, h3 {
    margin-left: 50px;
	text-align: left;
  }
  p {
    padding: 0 50px;
  }
  .image-container {
    display: flex;
    justify-content: space-between;
    margin-bottom: 20px;
  }
  .image-container img {
    width: 80%;
    height: auto;
    display: block;
    margin: 0 auto;
  }
  .caption {
	margin-top: 10px;
    text-align: center;
  }
  .image-container2 {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
  }
  .image-container2 img {
    width: 20%;
    height: auto;
    margin: 5px;
  }
</style>
</head>
<body>

<h1>Project 3: Pathtracer</h1>
<h2>Overview</h2>
<p></p>
<h2>Part 1: Ray Generation and Scene Intersection</h2>

<h3>Explanation of Ray Generation</h3>
<p> We begin by converting the pixel coordinates to represent the position within the image plane
  and are normalized to the range [0, 1], which is then transformed into camera coordinates.
  We use the function ('tan') to calculate horizontal and vertical offsets from the center of the 
  plane. Then the camera-to-world transformation matrix ('c2w') is applied to the camera coordinates
  so we can obtain the direction of the ray in world space. The resulting direction vector is normalized,
  the generated ray is constructed with its origin at the camera position ('pos'), and its direction is set
  to the computed ray direction. 
</p>

<h3>Explanation of Triangle/Sphere Intersection</h3>
<p> In the functions bool Triangle::has_intersection and bool Sphere::has_intersection, we are determining
  whether a ray intersects with a triangle using barycentric coordinates. In the class Triangle, it calculates the
  intersection point and checks if it's within the boundaries, returning true if so. While in Sphere, the 
  function checks whether a ray interects with a sphere by solving the quadratic equation derived from the
  ray's equation and the sphere's implicit equation.
</p>

<div class="image-container">
  <div>
    <img src="images/part1_ray_triangle.png" alt="Image 1">
    <div class="caption">Triangle.</div>
  </div>
  <div>
    <img src="images/part1_ray_sphere.png" alt="Image 2">
    <div class="caption">Sphere</div>
  </div>
  <div>
    <img src="images/part1_ray_coil.png" alt="Image 3">
    <div class="caption">Coil</div>
  </div>
</div>

<h2>Part 2: Bounding Volume Hierarchy</h2>
<h3>BVH Construction</h3>
<p>The algorithm recursively builds the BVH tree, and for each step we determine whether to create
	a leaf or internal node based on number of primitives and the maximum leaf size. If the number of
	nodes are less than or equal to the maximum leaf size or leaf node created, we enclose all the
	primitives within a bounding box. Otherwise, the internal node is created by splitting the primitives
	along the longest axis of their bounding box. The splitting point is chosen by finding the axis with
	the maximum extent and then sorting the primitives of their centroids along that axis. We keep doing this 
	until we reach the specified maximum leaf size.
</p>

<h3>Splitting Point Heuristic</h3>
<p>We pick the splitting point by balancing the number of primitives in each child node and minimizing
	the overlap between them. The algo selects the axis with the max extent along the three dimensions (x, y, z)
	of the bounding box. Then, it sorts the primitives based on their centroids along the axis and splits them
	into two groups, which makes sure each group has approximtaely equal number of primitives. 
</p>

<h3>Normal Shading</h3>
<p>By shading surfaces based on their surface normal, these images show how light interacts with the geometries 
	and highlight subtle variations in surface orientation. Normal shading enhances the realism of rendered scenes 
	thus simulate how light would act with surfaces.
</p>

<p> Below is the visualization of the BVH for 'meshedit/cow.dae' and shows how as we traverse down the root and in each
	level the BVH roughly halves the scene.
</p>

<div class="image-container2">
	<img src="images/part2_render_1.png" alt="Image 1">
	<img src="images/part2_render_2.png" alt="Image 2">
	<img src="images/part2_render_3.png" alt="Image 3">
	<img src="images/part2_render_4.png" alt="Image 4">
</div>
<div class="image-container2">
	<img src="images/part2_render_5.png" alt="Image 5">
	<img src="images/part2_render_6.png" alt="Image 6">
	<img src="images/part2_render_7.png" alt="Image 7">
	<img src="images/part2_render_8.png" alt="Image 8">
  </div>

  
<h3>Rendering Experiments</h3>
<p> Thus, with BVH it is much more efficient as we are reducing the naive approach of ray intersection
	computations. Thus, we are able to generate faster images that are still high-quality. The leftmost number is
	the without BVH and the rightmost number is with BVH acceleration.
</p>

<div class="image-container">
	<div>
	  <img src="images/part2_beast.png" alt="Image 1">
	  <div class="caption">19.9849s / 0.0488s, 0.0174 / 3.1435 million rays/sec, and 3172.602997 / 5.110071 intersection tests/ray.
	  </div>
	</div>
	<div>
	 <img src="images/part2_wall_e.png" alt="Image 2">
	 <div class="caption"> 90.8991s / 0.0704s, 0.0040 / 3.4442 million rays/sec, 7707.931367 / 10.198931 intersection tests/ray.
	 </div>
	</div>
	<div>
	  <img src="images/part2_CBlucy.png" alt="Image 3">
	  <div class="caption"> 46.4878s / 0.0442s, 0.0078 / 3.3153 million rays/sec, 3703.193595 / 5.585757 intersection tests/ray.
	  </div>
	</div>
</div>

<h2>Part 3: Direct Illumination</h2>
<h3>Diffuse BSDF</h3>
<p>In DiffuseBSDF::f, the function evaluates the diffuse Lambertian BSDF given the outgoing ('wo') and incident light directions ('wi').
	It returns the reflectance of these directions. In DiffuseBSDF::sample_f, we are taking the outgoing light direction and providing pointers
	for incoming directions and using the probability density function ('pdf') and returns the evaluation of BSDF at the outgoing light direction and
	incoming light.
</p>
<h3>Uniform hemisphere sampling</h3>
<p>The lighting is estimated by sampling uniformly in a hemisphere around the intersection point, and we are setting up a local coordinate system
	aligned with the surface normal at the point of intersection. We then sample directions in this local coordinate system and convert them to 
	the wiorld space for ray tracing. For each sample, a ray is cast in the sampled direction and if it intersects with the scene geometry then the 
	emitted radiance is accumulated based on the BSDF evaluation and the cosine term. However, this sampling method does not capture all the lighting
	effects accurately. We would need to take it a step further.
</p>
<h3>Light important sampling</h3>
<p>Instead of sampling from the light sources directly, we are doing so uniformly in a hemisphere. In PathTracer::estimate_direct_lighting_importance, 
	we iterate over each ligh source in the scene and sample points on the light surface, taking into account emitted radiance, distance attenuation,
	and PDF of sampling. For each sampled point on a light source, a shadow ray is cast towards the intersection point and if no intersection occurs along
	the ray then the radiance contribution is accumulated. This greatly reduces the noise and improves converging compared to uniform hemisphere sampling. Below
	are images to highlight key differences.
</p>
<h2>Part 4: Global Illumination</h2>
<!-- Content for Part 4 goes here -->

<h2>Part 5: Adaptive Sampling</h2>
<!-- Content for Part 5 goes here -->

</body>
</html>