<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Project 3: Pathtracer</title>
<style>
  body {
    margin: 20px;
  }
  h1, h2, h3 {
    margin-left: 50px;
	text-align: left;
  }
  p {
    padding: 0 50px;
  }
  .image-container {
    display: flex;
    justify-content: space-between;
    margin-bottom: 20px;
  }
  .image-container img {
    width: 80%;
    height: auto;
    display: block;
    margin: 0 auto;
}
	.caption {
		margin-top: 10px;
		text-align: center;
	}
  .image-container2 {
	  display: flex;
	  flex-wrap: wrap;
	  justify-content: center;
	}
	.image-container2 img {
		width: 20%;
		height: auto;
		margin: 5px;
	}

 .image-container3 {
    display: flex;
    gap: 20px; /* Adjust the gap between images and captions */
    margin-top: 20px;
  }
  .image-container3 .image-item {
    text-align: center;
  }
  .image-container3 img {
    max-width: 100%;
    height: auto;
  }
  .image-container3 .caption3 {
    margin-top: 10px;
	text-align: center;
  }
</style>
</head>
<body>

<h1>Project 3: Pathtracer</h1>
<h2>Overview</h2>
<p>In this project, we implemented the core methods necessary for simulating light to render realistic 3D scenes. I think it's the most interesting
	project that we've had so far, as we can see how the path of light rays from the virtual camera are simulated by each pixel of the image. We use 
	various geometric primitives (spheres and triangles), reflection and refraction, and other algorithms (sampling and shading). We implemented the 
	various computer graphics concepts such as ray-object intersection tests, Monte Carlo integration for global illumination, and importantance sampling.
	Furthermore, we also learned how to optimize the rendering of extremely detailed images to less than a minute, using bounding volume hierarchies
	for parallelization. Super cool project! Read more below to see all the cool images and learn about the more gritty details.
</p>

<h2>Part 1: Ray Generation and Scene Intersection</h2>

<h3>Explanation of Ray Generation</h3>
<p> We begin by converting the pixel coordinates to represent the position within the image plane
  and are normalized to the range [0, 1], which is then transformed into camera coordinates. The camera
  coordinates are (0,0), which is the bottom left corner of the plane, and (1,1), which is the top right.
  We use the function ('tan') to calculate horizontal and vertical offsets from the center of the 
  plane. Then the camera-to-world transformation matrix ('c2w') is applied to the camera coordinates
  so we can obtain the direction of the ray in world space. The resulting direction vector is normalized,
  the generated ray is constructed with its origin at the camera position ('pos'), and its direction is set
  to the computed ray direction. 
</p>

<div class="image-container">
	<div>
		<img src="images/ray_tracing.png" alt="Image 1">
    <div class="caption">Source: Lecture 9, 8th slide</div>
	</div>
</div>

<h3>Explanation of Triangle/Sphere Intersection</h3>
<p> First we begin by computing the triangle edges (p_1, p_2, p_3) and using the Moller_Trumbore algo we find the two edge vectors e_1 and e_2. 
	Using these values we find the cross product of the ray direction and the second edge (e_2) to find s_1. We then calculate the dot product 
	of s_1 and e_1. Using barycentric coordinates, we find the intersection of the ray with the triangle's plane. We have to check if it's within 
	the range of [0, 1], as explained earlier. If the intersection is within the range of min_t and max_t, which are the ray's valid ranges, then
	we can return the boolean true. Triangles are important for polygon rendering meshes.
</p>

<p>It's a little different for sphere intersection, as we need to start at the center and take into account the radius. We calculate the ray's origin
	to the sphere center (o - r). We use vector algebra and use the quadratic equation to find the intersection of the ray within the sphere. Similarly,
	if either one of these solutions fall within the valid range (min_t and max_t), then the function also returns true.
</p>

<div class="image-container">
  <div>
    <img src="images/part1_ray_triangle.png" alt="Image 1">
    <div class="caption">Triangle</div>
  </div>
  <div>
    <img src="images/part1_ray_sphere.png" alt="Image 2">
    <div class="caption">Sphere</div>
  </div>
  <div>
    <img src="images/part1_ray_coil.png" alt="Image 3">
    <div class="caption">Coil</div>
  </div>
</div>

<h2>Part 2: Bounding Volume Hierarchy</h2>
<h3>BVH Construction</h3>
<p> However, an issue that arises is that it takes too long to render the above images because of the many geometric
	primitives. Therefore, we recursively built the BVH tree, which is a binary tree that is interestingly built from 
	the bottom up. We initialize the BVHNode struct at the beginning. At each recursive step we determine whether to create
	a leaf or internal node based on number of primitives and the maximum leaf size. If the number of
	nodes are less than or equal to the maximum leaf size or leaf node created, we enclose all the
	primitives within a bounding box. Otherwise, the internal node is created by splitting the primitives
	along the longest axis of their bounding box. The splitting point is chosen by finding the axis with
	the maximum extent and then sorting the primitives of their centroids along that axis. We keep doing this 
	until we reach the specified maximum leaf size. We divide the left and right subtree so they don't overlap to help
	with efficiency.
</p>

<h3>Splitting Point Heuristic</h3>
<p>We pick the splitting point by balancing the number of primitives in each child node and minimizing
	the overlap between them. The algo selects the axis with the max extent along the three dimensions (x, y, z)
	of the bounding box. Then, it sorts the primitives based on their centroids along the axis and splits them
	into two groups, which makes sure each group has approximtaely equal number of primitives. However, if the pivot 
	point coincides with the bounding's box min or max, then we'll switch to the next axis to move onwards on finding the correct
	split point.
</p>


<p> Below is the visualization of the BVH for 'meshedit/cow.dae' and shows how as we traverse down the root and in each
	level the BVH roughly halves the scene.
</p>

<div class="image-container2">
	<img src="images/part2_render_1.png" alt="Image 1">
	<img src="images/part2_render_2.png" alt="Image 2">
	<img src="images/part2_render_3.png" alt="Image 3">
	<img src="images/part2_render_4.png" alt="Image 4">
</div>
<div class="image-container2">
	<img src="images/part2_render_5.png" alt="Image 5">
	<img src="images/part2_render_6.png" alt="Image 6">
	<img src="images/part2_render_7.png" alt="Image 7">
	<img src="images/part2_render_8.png" alt="Image 8">
  </div>

  <h3>Normal Shading</h3>
	<p>By shading surfaces based on their surface normal, these images show how light interacts with the geometries 
		and highlight subtle variations in surface orientation. Normal shading enhances the realism of rendered scenes 
		thus simulate how light would act with surfaces.
	</p>

	<div class="image-container">
		<div>
			<img src="images/part2_ugly_guy.png" alt="Image 1">
			<div class="caption">Rendering time: 0.1080s</div>
		</div>
		<div>
			<img src="images/part2_peter.png" alt="Image 2">
			<div class="caption">Rendering time: 0.0649s</div>
		</div>
		<div>
			<img src="images/part2_building.png" alt="Image 3">
			<div class="caption">Rendering time: 0.0444s</div>
		</div>
		</div>
  
<h3>Rendering Experiments</h3>
<p> Thus, with BVH it is much more efficient as we are reducing the naive approach of ray intersection
	computations We are able to generate faster images that are still high-quality. 
</p>

<p>The leftmost number is implemented without BVH and the rightmost number is with BVH acceleration.</p>
<div class="image-container">
	<div>
	  <img src="images/part2_beast.png" alt="Image 1">
	  <div class="caption">19.9849s / 0.0488s, 0.0174 / 3.1435 million rays/sec, and 3172.602997 / 5.110071 intersection tests/ray.
	  </div>
	</div>
	<div>
	 <img src="images/part2_wall_e.png" alt="Image 2">
	 <div class="caption"> 90.8991s / 0.0704s, 0.0040 / 3.4442 million rays/sec, 7707.931367 / 10.198931 intersection tests/ray.
	 </div>
	</div>
	<div>
	  <img src="images/part2_CBlucy.png" alt="Image 3">
	  <div class="caption"> 46.4878s / 0.0442s, 0.0078 / 3.3153 million rays/sec, 3703.193595 / 5.585757 intersection tests/ray.
	  </div>
	</div>
</div>

<h2>Part 3: Direct Illumination</h2>
<h3>Diffuse BSDF</h3>
<p>In DiffuseBSDF::f, the function evaluates the diffuse Lambertian BSDF given the outgoing ('wo') and incident light directions ('wi').
	It returns the reflectance of these directions. In DiffuseBSDF::sample_f, we are taking the outgoing light direction and providing pointers
	for incoming directions and using the probability density function ('pdf') and returns the evaluation of BSDF at the outgoing light direction and
	incoming light.
</p>
<h3>Uniform hemisphere sampling</h3>
<p>The lighting is estimated by sampling uniformly in a hemisphere around the intersection point, and we are setting up a local coordinate system
	aligned with the surface normal at the point of intersection. We then sample directions in this local coordinate system and convert them to 
	the wiorld space for ray tracing. For each sample, a ray is cast in the sampled direction and if it intersects with the scene geometry then the 
	emitted radiance is accumulated based on the BSDF evaluation and the cosine term. However, this sampling method does not capture all the lighting
	effects accurately. We would need to take it a step further.
</p>
<h3>Light important sampling</h3>
<p>Instead of sampling from the light sources directly, we are doing so uniformly in a hemisphere. In PathTracer::estimate_direct_lighting_importance, 
	we iterate over each ligh source in the scene and sample points on the light surface, taking into account emitted radiance, distance attenuation,
	and PDF of sampling. For each sampled point on a light source, a shadow ray is cast towards the intersection point and if no intersection occurs along
	the ray then the radiance contribution is accumulated. This greatly reduces the noise and improves converging compared to uniform hemisphere sampling. Below
	are images to highlight key differences.
</p>

<p>Now we can render scenes with less noise and handle lighting from the light sources directly. Shown below are files with 64 camera rays per pixels and 
	32 samples per area of light.
</p>
<div class="image-container">
	<div>
	  <img src="images/part3_dragon.png" alt="Image 1">
	</div>
	<div>
	 <img src="images/part3_wall_e.png" alt="Image 2">
	</div>
	<div>
		<img src="images/part3_bunny.png" alt="Image 3">
	</div>
</div>

<p>Below, we have the different noise levels in soft shadows in CBbunny.dae.</p>

<div class="image-container3">
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_1_1.png" alt="Image 1">
	  <div class="caption3">Samples: 1</div>
	</div>
	<div class="image-item3">
		<img src="images/part3_bunny_4_4.png" alt="Image 2">
		<div class="caption3">Samples: 4</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_16_8.png" alt="Image 3">
	  <div class="caption3">Samples: 16</div>
	</div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_64_32.png" alt="Image 4">
	  <div class="caption3">Samples: 64</div>
	</div>
  </div>
  
  <div class="image-container3">
	<div class="image-item3">
	  <img src="images/part3_CBbunny_1_1.png" alt="Image 5">
	  <div class="caption3">Light rays: 1</div>
	</div>
	<div class="image-item3">
		<img src="images/part3_bunny_1_4.png" alt="Image 6">
		<div class="caption3">Light rays: 4</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_16_16.png" alt="Image 7">
	  <div class="caption3">Light rays: 16</div>
	</div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_64_64.png" alt="Image 8">
	  <div class="caption3">Light rays: 64</div>
	</div>
  </div>
<h2>Part 4: Global Illumination</h2>
<!-- Content for Part 4 goes here -->

<h2>Part 5: Adaptive Sampling</h2>
<!-- Content for Part 5 goes here -->

</body>
</html>