<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Project 3: Pathtracer</title>
<style>
  body {
    margin: 20px;
  }
  h1, h2, h3 {
    margin-left: 50px;
	text-align: left;
  }
  p {
    padding: 0 50px;
  }
  .image-container {
    display: flex;
    justify-content: space-between;
    margin-bottom: 20px;
  }
  .image-container img {
    width: 80%;
    height: auto;
    display: block;
    margin: 0 auto;
}
	.caption {
		margin-top: 10px;
		text-align: center;
	}
  .image-container2 {
	  display: flex;
	  flex-wrap: wrap;
	  justify-content: center;
	}
	.image-container2 img {
		width: 20%;
		height: auto;
		margin: 5px;
	}

 .image-container3 {
    display: flex;
    gap: 20px; /* Adjust the gap between images and captions */
    margin-top: 20px;
  }
  .image-container3 .image-item {
    text-align: center;
  }
  .image-container3 img {
    max-width: 100%;
    height: auto;
  }
  .image-container3 .caption3 {
    margin-top: 10px;
	text-align: center;
  }
</style>
</head>
<body>

<h1>Project 3: Pathtracer</h1>
<h2>Overview</h2>
<p>In this project, we implemented the core methods necessary for simulating light to render realistic 3D scenes. I think it's the most interesting
	project that we've had so far, as we can see how the path of light rays from the virtual camera are simulated by each pixel of the image. We use 
	various geometric primitives (spheres and triangles), reflection and refraction, and other algorithms (sampling and shading). We implemented the 
	various computer graphics concepts such as ray-object intersection tests, Monte Carlo integration for global illumination, and importantance sampling.
	Furthermore, we also learned how to optimize the rendering of extremely detailed images to less than a minute, using bounding volume hierarchies
	for parallelization. Super cool project!
</p>

<h2>Part 1: Ray Generation and Scene Intersection</h2>

<h3>Explanation of Ray Generation</h3>
<p> We begin by converting the pixel coordinatesm, which we simply found by subtracting 0.5 by the x and y,
  to represent the position within the image plane
  and are normalized to the range [0, 1], which is then transformed into camera coordinates. The camera
  coordinates are (0,0), which is the bottom left corner of the plane, and (1,1), which is the top right.
  We use the function ('tan') to calculate horizontal and vertical offsets from the center of the 
  plane. Then the camera-to-world transformation matrix ('c2w') is applied to the camera coordinates
  so we can obtain the direction of the ray in world space. The resulting direction vector is normalized,
  the generated ray is constructed with its origin at the camera position ('pos'), and its direction is set
  to the computed ray direction. 
</p>

<div class="image-container2">
	<div class="image-item3">
		<div>
			<img src="images/ray_tracing.png" alt="Image 1">
		<div class="caption">Source: Lecture 9, 8th slide</div>
		</div>
	</div>
</div>

<h3>Explanation of Triangle/Sphere Intersection</h3>
<p> First we begin by computing the triangle edges (p_1, p_2, p_3) and using the Moller_Trumbore algo we find the two edge vectors e_1 and e_2. 
	Using these values we find the cross product of the ray direction and the second edge (e_2) to find s_1. We then calculate the dot product 
	of s_1 and e_1. Using barycentric coordinates, we find the intersection of the ray with the triangle's plane. We have to check if it's within 
	the range of [0, 1], as explained earlier. If the intersection is within the range of min_t and max_t, which are the ray's valid ranges, then
	we can return the boolean true. Triangles are important for polygon rendering meshes.
</p>

<p>It's a little different for sphere intersection, as we need to start at the center and take into account the radius. We calculate the ray's origin
	to the sphere center (o - r). We use vector algebra and use the quadratic equation to find the intersection of the ray within the sphere. Similarly,
	if either one of these solutions fall within the valid range (min_t and max_t), then the function also returns true.
</p>

<div class="image-container">
  <div>
    <img src="images/part1_ray_triangle.png" alt="Image 1">
    <div class="caption">Triangle</div>
  </div>
  <div>
    <img src="images/part1_ray_sphere.png" alt="Image 2">
    <div class="caption">Sphere</div>
  </div>
  <div>
    <img src="images/part1_ray_coil.png" alt="Image 3">
    <div class="caption">Coil</div>
  </div>
</div>

<h2>Part 2: Bounding Volume Hierarchy</h2>
<h3>BVH Construction</h3>
<p> However, an issue that arises is that it takes too long to render the above images because of the many geometric
	primitives. Therefore, we recursively built the BVH tree, which is a binary tree that is interestingly built from 
	the bottom up. We initialize the BVHNode struct at the beginning. At each recursive step we determine whether to create
	a leaf or internal node based on number of primitives and the maximum leaf size. If the number of
	nodes are less than or equal to the maximum leaf size or leaf node created, we enclose all the
	primitives within a bounding box. Otherwise, the internal node is created by splitting the primitives
	along the longest axis of their bounding box. The splitting point is chosen by finding the axis with
	the maximum extent and then sorting the primitives of their centroids along that axis. We keep doing this 
	until we reach the specified maximum leaf size. We divide the left and right subtree so they don't overlap to help
	with efficiency.
</p>

<p>We also set the BBox to help with intersection, as it checks if a ray intersects the bounding box. We check if the ray doesn't intersect 
	with the max of the mins and the mins and the maxes of the boundaries, but if it does then we are able to calculate the intersection of the 
	intervals.
</p>
<h3>Splitting Point Heuristic</h3>
<p>We pick the splitting point by balancing the number of primitives in each child node and minimizing
	the overlap between them. The algo selects the axis with the max extent along the three dimensions (x, y, z)
	of the bounding box. Then, it sorts the primitives based on their centroids along the axis and splits them
	into two groups, which makes sure each group has approximtaely equal number of primitives. However, if the pivot 
	point coincides with the bounding's box min or max, then we'll switch to the next axis to move onwards on finding the correct
	split point.
</p>


<p> Below is the visualization of the BVH for 'meshedit/cow.dae' and shows how as we traverse down the root and in each
	level the BVH roughly halves the scene.
</p>

<div class="image-container2">
	<img src="images/part2_render_1.png" alt="Image 1">
	<img src="images/part2_render_2.png" alt="Image 2">
	<img src="images/part2_render_3.png" alt="Image 3">
	<img src="images/part2_render_4.png" alt="Image 4">
</div>
<div class="image-container2">
	<img src="images/part2_render_5.png" alt="Image 5">
	<img src="images/part2_render_6.png" alt="Image 6">
	<img src="images/part2_render_7.png" alt="Image 7">
	<img src="images/part2_render_8.png" alt="Image 8">
  </div>

  <h3>Normal Shading</h3>
	<p>By shading surfaces based on their surface normal, these images show how light interacts with the geometries 
		and highlight subtle variations in surface orientation. Normal shading enhances the realism of rendered scenes 
		thus simulate how light would act with surfaces.
	</p>

	<div class="image-container3">
		<div>
			<img src="images/part2_ugly_guy.png" alt="Image 1">
			<div class="caption">Rendering time: 0.1080s</div>
		</div>
		<div>
			<img src="images/part2_peter.png" alt="Image 2">
			<div class="caption">Rendering time: 0.0649s</div>
		</div>
		<div>
			<img src="images/part2_building.png" alt="Image 3">
			<div class="caption">Rendering time: 0.0444s</div>
		</div>
		</div>
  
<h3>Rendering Experiments</h3>

<p>The leftmost number is implemented without BVH and the rightmost number is with BVH acceleration.</p>
<div class="image-container">
	<div>
		<img src="images/part2_beast.png" alt="Image 1">
		<div class="caption"> Rendering time: 19.9849s / 0.0488s
		</div>
		<div class="caption"> Rays: 0.0174 / 3.1435 million rays/sec </div>
		<div class="caption"> Intersections: 3172.602997 / 5.110071 intersection tests/ray </div>
	</div>
	<div>
		<img src="images/part2_wall_e.png" alt="Image 2">
		<div class="caption"> Rendering time: 90.8991s / 0.0704s</div>
		<div class="caption"> Rays: 0.0040 / 3.4442 million rays/sec </div>
		<div class="caption"> Intersections: 7707.931367 / 10.198931 intersection tests/ray </div>
	</div>
	<div>
		<img src="images/part2_CBlucy.png" alt="Image 3">
		<div class="caption"> Rendering time: 46.4878s / 0.0442s</div>
		<div class="caption"> Rays: 3703.193595 / 5.585757 intersection tests/ray </div>
		<div class="caption"> Intersections: 13234.2123 / 6.12321 intersection tests/ray </div>
	</div>
</div>

<p> Thus, with BVH it is much more efficient as we are reducing the naive approach of ray intersection
	computations We are able to generate faster images that are still high-quality, especially those with 
	many primitives, which are the ones shown above. However, I noticed that with CBsphere, which only has 14
	primitives, the rendering time did not have that big of a different. It was around the same time which was like 0.300 seconds.
	A linear scan works fine, since BVH wouldn't be able to cut that much time with those with smaller primitives like that anyways.
	It was very interesting to see .dae files that had up to like 20,000 primitives be rendered to less than a second due to the 
	binary k tree.
</p>
<h2>Part 3: Direct Illumination</h2>
<h3>Diffuse BSDF</h3>
<p> To model the distribution of light at each intersection, we weighed the illuminance by using BSDF (bidirectional reflectance distribution function).
	The function checks if there is light coming in and coming out of a particular direction, which was one of the given variables.
	In DiffuseBSDF::f, the function evaluates the diffuse Lambertian BSDF given the outgoing ('wo') and incident light directions ('wi').
	It returns the reflectance of these directions. In DiffuseBSDF::sample_f, we are taking the outgoing light direction and providing pointers
	for incoming directions and using the probability density function ('pdf') and returns the evaluation of BSDF at the outgoing light direction and
	incoming light.
</p>
<h3>Uniform hemisphere sampling</h3>
<p>The lighting is estimated by sampling uniformly in a hemisphere around the intersection point, and we are setting up a local coordinate system
	aligned with the surface normal at the point of intersection. We then sample directions in this local coordinate system and convert them to 
	the wiorld space for ray tracing. The directions were weighted by the cosine of the angle between the sampled directions and the normla. This is because 
	light arriving at angles have less brightness than if it were to be cast directly perpendicular, which has the nice effect of a gradient of color.
	For each sample, a ray is cast in the sampled direction and if it intersects with the scene geometry then the 
	emitted radiance is accumulated based on the BSDF evaluation, which checks how much light is scattered and reflected. 
</p>
<p>However, this sampling method does not capture all the lighting effects accurately. We would need to take it a step further.
	For instance, it does not capture the more complex reflections and that's why we would need importance sampling or the Monte Carlo integration 
	which was discussed in lecture. 
</p>
<p>Below are uniform hemisphere sampling images, and now we are able to include shadows and it's more obvious that the light is more bright at the 
	top of the bunny.</p>

<div class="image-container3">
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_1_1.png" alt="Image 1">
	  <div class="caption3">Samples: 1</div>
	</div>
	<div class="image-item3">
	  <img src="images/part3_bunny_4_hemisphere.png" alt="Image 3">
	  <div class="caption3">Samples: 4</div>
	</div>
	<div class="image-item3">
		<img src="images/part3_CBbunny_hemisphere_16_8.png" alt="Image 2">
		<div class="caption3">Samples: 16</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_hemisphere_64_32.png" alt="Image 4">
	  <div class="caption3">Samples: 64</div>
	</div>
  </div>

  
  <h3>Light important sampling</h3>
  <p>Instead of sampling from the light sources uniformly on a hemisphere, we are now doing it directly. In PathTracer::estimate_direct_lighting_importance, 
	  we iterate over each light source in the scene and sample points on the light surface, taking into account emitted radiance, distance attenuation,
	  and PDF of sampling. We get the sample by taking the hit point of the light, which is a given constant. We also add the offset of EPS_D times the direction of 
	  light to see what the shadow ray would be intersecting with. If it's intersecting with nothing then we would add this weighted sample to L_out.
	  This greatly reduces the noise and improves converging compared to uniform hemisphere sampling, so images look much more smooth.
	</p>
	
  <p>Below are light important sampling images.</p>

<div class="image-container3">
	<div class="image-item3">
	  <img src="images/part3_importance_bunny_1_1.png" alt="Image 1">
	  <div class="caption3">Samples: 1</div>
	</div>
	<div class="image-item3">
	  <img src="images/part3_4_importance.png" alt="Image 3">
	  <div class="caption3">Samples: 4</div>
	</div>
	<div class="image-item3">
		<img src="images/part3_importance_16.png" alt="Image 2">
		<div class="caption3">Samples: 16</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part3_importance_CBbunny_H_64_32.png" alt="Image 4">
	  <div class="caption3">Samples: 64</div>
	</div>
  </div>

<p>Now we can render scenes with less noise and handle lighting from the light sources directly. Shown below are files with 64 camera rays per pixels and 
	32 samples per area of light.
</p>
<div class="image-container">
	<div>
	  <img src="images/part3_dragon.png" alt="Image 1">
	</div>
	<div>
	 <img src="images/part3_wall_e.png" alt="Image 2">
	</div>
	<div>
		<img src="images/part3_bunny.png" alt="Image 3">
	</div>
</div>

<p>Below, we have the different noise levels in soft shadows in CBbunny.dae. This is done by returning one_bounce_radiance in 'PathTracer::est_radiance_global_illumination.'</p>

<div class="image-container3">
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_1_1.png" alt="Image 1">
	  <div class="caption3">Samples: 1</div>
	</div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_16_8.png" alt="Image 3">
	  <div class="caption3">Samples: 4</div>
	</div>
	<div class="image-item3">
		<img src="images/part3_bunny_4_4.png" alt="Image 2">
		<div class="caption3">Samples: 16</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_64_32.png" alt="Image 4">
	  <div class="caption3">Samples: 64</div>
	</div>
  </div>
  
  <div class="image-container3">
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_1_1.png" alt="Image 5">
	  <div class="caption3">Light rays: 1</div>
	</div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_16_16.png" alt="Image 7">
	  <div class="caption3">Light rays: 4</div>
	</div>
	<div class="image-item3">
		<img src="images/part3_bunny_1_4.png" alt="Image 6">
		<div class="caption3">Light rays: 16</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part3_CBbunny_H_64_64.png" alt="Image 8">
	  <div class="caption3">Light rays: 64</div>
	</div>
  </div>

<h2>Part 4: Global Illumination</h2>

<p>We compute the radiance that accounts for light interaction, whether through direct
	or indirect lighting from one or more bounces. In the previous functions, we consider all the possible paths 
	that light takes going from a light source without reflections. However, now we are considering both direct 
	and indirect lighting that interact with the surface of the scene. We initialize L_out (illumination output) to be
	one_bounce_radiance, which is the direct lighting. To compute the final radiance, we use 
	an interesting concept called the "russian roulette" as it helps in reducing biasing to determine whether to include
	contributions from indirecting lighting or to terminate recursion early based on probability threshold (0.3). 
	For some reason, it's much more grainy when I did trial and error and chose a higher threshold (0.4), so I settled with
	that number. We keep recursing down and continue casting the new ray until we hit a max_depth of 1, and as we're going down each level we're taking the vector of
	the 'bounceSample' and multiplying it by 'bsdf,' 'pdf,' world space coordinates, and etc. We're then adding that weighted sample to the output. I 
	also divided it by the russian roulette probability. It was very cool to see how my favorite girl group (Red Velvet) helped with 
	global illumination and reflections, leading to more realistic and visually appealing renders.
</p>

<h3>Indirect, Direct Illumination Comparison: 'CBsphere.dae'</h3>
<div class="image-container3">
	<div class="image-item3">
		<img src="images/part4_direct.png" alt="Image 2">
		<div class="caption3">Direct is when there is one bounce.</div>
	</div>
	<div class="image-item3">
		<img src="images/part4_spheres_5.png" alt="Image 1">
		<div class="caption3">Indirect is when there is greater than one. </div>
	  </div>
</div>

<h3>Max-ray-depth Comparison: 'bunny.dae'</h3>
<p>This is with 64 samples and 32 light rays, and notice how the room is increasingly brighter with a greater max ray depth. There 
	is a great jump in the color form a ray depth of 1, as that is just direct lighting and anything greater than that is indirect lighting.
	You can see that the light is more scattered in ray depths of 1, 5, and 10.
</p>
<div class="image-container3">
	<div class="image-item3">
		<img src="images/part4_bunny_0.png" alt="Image 1">
		<div class="caption3">max_ray_depth = 0</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part4_bunny_5.png" alt="Image 2">
	  <div class="caption3">max_ray_depth = 1</div>
	</div>
	<div class="image-item3">
		<img src="images/part4_bunny_8.png" alt="Image 4">
		<div class="caption3">max_ray_depth = 8</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part4_bunny_15.png" alt="Image 5">
	  <div class="caption3">max_ray_depth = 15</div>
	</div>
  </div>
<h3>Sample-per-pixel Comparison: 'sphere.dae'</h3>
<p> Look at how the spheres for a larger samples per pixel size (64 and 1024) are so satisfying to look at. However, the 1024 
	size took around 15 or 20 minutes for it fully render and the 64 size one also took a very long time. The tradeoff for time
	for a smoother look might be annoying, but smaller sample sizes (less than 10) have such a grainy look to them.
</p>
<div class="image-container3">
	<div class="image-item3">
		<img src="images/part4_spheres_1.png" alt="Image 1">
		<div class="caption3">Samples per pixel: 1</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part4_spheres_8.png" alt="Image 2">
	  <div class="caption3">Samples per pixel: 8</div>
	</div>
	<div class="image-item3">
		<img src="images/part4_spheres_5.png" alt="Image 4">
		<div class="caption3">Samples per pixel: 64</div>
	  </div>
	<div class="image-item3">
	  <img src="images/part4_spheres_1024.png" alt="Image 5">
	  <div class="caption3">Samples per pixel = 1024</div>
	</div>
  </div>
<h2>Part 5: Adaptive Sampling</h2>
<p>Adaptive sampling dynamically adjusts the number of samples taken per pixel based on convergence of the rendered image.
	If only only one sample is required, a single ray is traced through the center of the pixel. There is no need for adaptive sampling, and we can just
	generate illumination as is. We set the buffer to be just 1 and then set the pixel as that. Otherwise, we enter the for loop 
	and iterate through the specified number of samples, by checkin the number of pixels in a batch and if it's not a multiple of it then we are to surmise that
	it's not in the first batch. In each iteration, a new ray is set and accumulated. We find the mean and variance 
	of the accumulated samples to estimate the error of the image, and if it falls within the threshold (maxTolerance * mean), then we are done sampling and exit the loop.
	We then generate the rays and trace them through the scene, taking the average results of all the samples and storing it in a sample buffer. The algorithm helps optimize rendering 
	efficiency by dynamically adjusting the number of samples and placing them in the buffer, based off of the convergence rate of rendered image. It also greatly reminds of adaptive helm,
	which is my favorite TFT item.
</p>

</body>
</html>